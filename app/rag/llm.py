from functools import lru_cache
from langchain.llms import Ollama

@lru_cache(maxsize=1)
def load_llm(model_name: str = "Meta-Llama-3-Ko-Instruct-8B", temperature: float = 0.2):
    """
    Ollama LLM ë¡œë” (ìµœì´ˆ 1íšŒë§Œ ì´ˆê¸°í™”; ì´í›„ ìºì‹œ ì‚¬ìš©)
    - model_name: ollama list ë¡œ í™•ì¸ ê°€ëŠ¥í•œ ë¡œì»¬ ëª¨ë¸ëª…
    """
    print("ğŸš€ LLM ëª¨ë¸ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒ)")
    return Ollama(
        model=model_name,
        system=(
            "ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
            "ë¬¸ì„œ ê·¼ê±° ë²”ìœ„ ë‚´ì—ì„œë§Œ ë‹µë³€í•˜ê³ , ë¶ˆí•„ìš”í•œ ì˜ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."
        ),
        temperature=temperature,
        num_ctx=4096,
    )
